{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fuyu-quant/IBLM/blob/main/examples/iblbagging/pseudodata_openai.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install iblm --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from iblm import IBLBaggingModel\n",
    "\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_data = 10000 # 50〜300\n",
    "num_test_data = 1000\n",
    "sample = num_train_data + num_test_data\n",
    "n_informative = 2\n",
    "n_redundant = 0\n",
    "n_features = n_informative + n_redundant\n",
    "weights = [0.5, 0.5]\n",
    "flip_y=0\n",
    "seed = 3655  # 3655,3656,3657\n",
    "\n",
    "# testデータの個数を揃えるためにtrain_test_splitを使っていない\n",
    "x, y = make_classification(\n",
    "    n_samples = sample,  # データ数\n",
    "    n_features = n_features,  # 特徴量の数\n",
    "    n_informative = n_informative,  # ラベル予測に意味のある特徴量の数\n",
    "    n_redundant = n_redundant,  # 冗長な特徴量\n",
    "    weights = weights,  # [0,1]の割合\n",
    "    flip_y = flip_y, # 逆のラベルに反転する割合\n",
    "    random_state = seed\n",
    "    )\n",
    "\n",
    "x = np.round(x, decimals=3)\n",
    "x = pd.DataFrame(x)\n",
    "y = y.astype(int)\n",
    "\n",
    "x_test = x[0:num_test_data]\n",
    "x_train = x[num_test_data:]\n",
    "\n",
    "y_test = y[:num_test_data]\n",
    "y_train = y[num_test_data:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBLbagging setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iblbagging = IBLBaggingModel(\n",
    "    api_type=\"openai\",\n",
    "    model_name=\"gpt-4-0125-preview\",\n",
    "    objective=\"binary\",\n",
    "    num_model=20,\n",
    "    max_sample = 2000,\n",
    "    min_sample = 300,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 18:41:38,941 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:41:47,975 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:42:01,886 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:42:13,703 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:42:28,133 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:42:46,786 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:43:00,170 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:43:10,694 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:43:20,536 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:43:39,094 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:43:49,498 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:44:00,119 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:44:15,677 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:44:26,336 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:44:40,737 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:45:06,749 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:45:18,239 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:45:28,519 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:45:42,373 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n",
      "2024-03-10 18:45:51,139 [iblm.ibl][INFO] (ibl:ibl.py:fit:153)\n"
     ]
    }
   ],
   "source": [
    "code_models = iblbagging.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    temperature=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model_2',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n        \\n        # Simple logistic regression coefficients, manually estimated from the dataset\\n        # These values are placeholders and should ideally be determined using a logistic regression model fitting\\n        # However, as per the instructions, we are not using any existing machine learning models for predictions.\\n        intercept = 0.5  # Intercept (bias)\\n        coef_0 = 0.25  # Coefficient for the first feature\\n        coef_1 = 0.75  # Coefficient for the second feature\\n        \\n        # Logistic regression formula to estimate probability\\n        # p = 1 / (1 + e^-(intercept + coef_0*x0 + coef_1*x1))\\n        log_odds = intercept + coef_0*row[0] + coef_1*row[1]\\n        probability = 1 / (1 + np.exp(-log_odds))\\n        \\n        y = probability\\n\\n        # Do not change the code after this point.\\n        output.append(y)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.8852,\n",
       "    'pr_auc': 0.859633,\n",
       "    'accuracy': 0.781732,\n",
       "    'recall': 0.930556,\n",
       "    'precision': 0.723022,\n",
       "    'f1_score': 0.813765}}),\n",
       " ('model_7',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n        \\n        # Simple logistic regression coefficients derived from the dataset manually or through another script\\n        # These coefficients are placeholders and should be replaced with actual values derived from the dataset\\n        intercept = 0.5  # Placeholder intercept\\n        coef_0 = 0.1  # Placeholder coefficient for the first feature\\n        coef_1 = 0.2  # Placeholder coefficient for the second feature\\n        \\n        # Logistic regression formula\\n        log_odds = intercept + coef_0 * row[0] + coef_1 * row[1]\\n        odds = np.exp(log_odds)\\n        y = odds / (1 + odds)\\n\\n        # Do not change the code after this point.\\n        output.append(y)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.867082,\n",
       "    'pr_auc': 0.805406,\n",
       "    'accuracy': 0.573892,\n",
       "    'recall': 0.997561,\n",
       "    'precision': 0.54244,\n",
       "    'f1_score': 0.702749}}),\n",
       " ('model_20',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n        \\n        # Simple logistic regression coefficients derived from manual observation and analysis\\n        # These coefficients are placeholders and should ideally be determined using a logistic regression model\\n        # trained on the dataset. However, as per the instructions, we are not using any existing ML model.\\n        coef_0 = 0.5  # Intercept\\n        coef_1 = 0.1  # Coefficient for the first feature\\n        coef_2 = 0.2  # Coefficient for the second feature\\n        \\n        # Logistic regression equation\\n        z = coef_0 + (coef_1 * row[0]) + (coef_2 * row[1])\\n        \\n        # Sigmoid function to map the linear regression output to a probability\\n        y = 1 / (1 + np.exp(-z))\\n\\n        # Do not change the code after this point.\\n        output.append(y)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.862793,\n",
       "    'pr_auc': 0.808174,\n",
       "    'accuracy': 0.56892,\n",
       "    'recall': 1.0,\n",
       "    'precision': 0.540692,\n",
       "    'f1_score': 0.701882}}),\n",
       " ('model_14',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n        \\n        # Simple logistic regression coefficients derived from manual analysis\\n        # These coefficients are placeholders and do not represent a trained model\\n        coef_0 = 0.5  # Intercept\\n        coef_1 = 0.1  # Coefficient for the first feature\\n        coef_2 = 0.2  # Coefficient for the second feature\\n        \\n        # Logistic regression formula\\n        z = coef_0 + (coef_1 * row[0]) + (coef_2 * row[1])\\n        y = 1 / (1 + np.exp(-z))  # Sigmoid function to predict probability\\n\\n        # Do not change the code after this point.\\n        output.append(y)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.862738,\n",
       "    'pr_auc': 0.816967,\n",
       "    'accuracy': 0.587302,\n",
       "    'recall': 1.0,\n",
       "    'precision': 0.548183,\n",
       "    'f1_score': 0.708163}}),\n",
       " ('model_9',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n        \\n        # Simple logistic regression coefficients derived from manual observation and analysis\\n        # These coefficients are placeholders and not derived from actual logistic regression analysis\\n        # They are for demonstration purposes only\\n        intercept = 0.5\\n        coef_col0 = 0.1\\n        coef_col1 = 0.2\\n        \\n        # Logistic regression formula\\n        log_odds = intercept + coef_col0 * row[0] + coef_col1 * row[1]\\n        odds = np.exp(log_odds)\\n        y = odds / (1 + odds)\\n\\n        # Do not change the code after this point.\\n        output.append(y)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.859456,\n",
       "    'pr_auc': 0.787889,\n",
       "    'accuracy': 0.569912,\n",
       "    'recall': 1.0,\n",
       "    'precision': 0.53626,\n",
       "    'f1_score': 0.698137}}),\n",
       " ('model_18',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Feature engineering and simple logistic regression-like model\\n        # Coefficients are manually derived for simplicity and demonstration purposes\\n        coef_0 = 0.5\\n        coef_1 = 1.2\\n        coef_2 = -0.8\\n        intercept = 0.05\\n\\n        # Simple logistic function to calculate probability\\n        z = intercept + coef_0*row[0] + coef_1*row[1] + coef_2\\n        probability = 1 / (1 + np.exp(-z))\\n        \\n        y = probability\\n        output.append(y)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.856997,\n",
       "    'pr_auc': 0.809939,\n",
       "    'accuracy': 0.752203,\n",
       "    'recall': 0.641892,\n",
       "    'precision': 0.811966,\n",
       "    'f1_score': 0.716981}}),\n",
       " ('model_1',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n\\n        # Simple logistic regression coefficients derived from the data\\n        # These coefficients are placeholders and should be replaced with actual values derived from analysis\\n        intercept = 0.5  # Placeholder intercept\\n        coef_0 = 0.1  # Placeholder coefficient for the first feature\\n        coef_1 = 0.2  # Placeholder coefficient for the second feature\\n\\n        # Logistic regression formula\\n        z = intercept + coef_0*row[0] + coef_1*row[1]\\n        y = 1 / (1 + np.exp(-z))  # Sigmoid function\\n\\n        # Do not change the code after this point.\\n        output.append(y)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.855325,\n",
       "    'pr_auc': 0.793741,\n",
       "    'accuracy': 0.581301,\n",
       "    'recall': 0.997992,\n",
       "    'precision': 0.547357,\n",
       "    'f1_score': 0.70697}}),\n",
       " ('model_17',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n        \\n        # Simple logistic regression coefficients, manually estimated from data\\n        # These are dummy values and should be replaced with actual model coefficients\\n        intercept = 0.5\\n        coef_0 = 0.1\\n        coef_1 = 0.2\\n        \\n        # Calculate the linear combination of inputs and coefficients\\n        linear_combination = intercept + coef_0 * row[0] + coef_1 * row[1]\\n        \\n        # Apply the sigmoid function to get the probability\\n        y = 1 / (1 + np.exp(-linear_combination))\\n\\n        # Do not change the code after this point.\\n        output.append(y)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.854971,\n",
       "    'pr_auc': 0.795779,\n",
       "    'accuracy': 0.572387,\n",
       "    'recall': 0.994,\n",
       "    'precision': 0.528723,\n",
       "    'f1_score': 0.690278}}),\n",
       " ('model_4',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n\\n        # Simple logistic regression coefficients, manually derived for demonstration.\\n        # These values are placeholders. In a real scenario, you would derive these\\n        # from training data using a logistic regression model.\\n        coef_0 = 0.5  # Intercept\\n        coef_1 = 0.1  # Coefficient for the first feature\\n        coef_2 = 0.2  # Coefficient for the second feature\\n\\n        # Logistic regression formula to calculate the log-odds\\n        log_odds = coef_0 + coef_1 * row[0] + coef_2 * row[1]\\n\\n        # Sigmoid function to convert log-odds to probability\\n        probability = 1 / (1 + np.exp(-log_odds))\\n\\n        # Do not change the code after this point.\\n        output.append(probability)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.854092,\n",
       "    'pr_auc': 0.814775,\n",
       "    'accuracy': 0.597037,\n",
       "    'recall': 1.0,\n",
       "    'precision': 0.568254,\n",
       "    'f1_score': 0.724696}}),\n",
       " ('model_12',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n        \\n        # Simple logistic regression coefficients derived from the dataset manually or by observation\\n        # Intercept\\n        b0 = 0.5\\n        # Coefficients for the features\\n        b1, b2 = 0.1, 0.15\\n        \\n        # Logistic regression formula\\n        z = b0 + (b1 * row[0]) + (b2 * row[1])\\n        y = 1 / (1 + np.exp(-z))\\n\\n        # Do not change the code after this point.\\n        output.append(y)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.831115,\n",
       "    'pr_auc': 0.752154,\n",
       "    'accuracy': 0.514445,\n",
       "    'recall': 1.0,\n",
       "    'precision': 0.5,\n",
       "    'f1_score': 0.666667}}),\n",
       " ('model_16',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n        \\n        # Simple linear model based on observed data patterns\\n        # This is a placeholder for the actual logic you would need to implement\\n        # based on the analysis of the given dataset.\\n        # For demonstration purposes, we use a simple condition on the features.\\n        if row[0] > 0 and row[1] > 0:\\n            y = 0.8  # Higher probability of being 1 if both features are positive\\n        elif row[0] < 0 and row[1] > 0:\\n            y = 0.7  # Slightly lower probability if the first feature is negative\\n        elif row[0] > 0 and row[1] < 0:\\n            y = 0.3  # Lower probability if the second feature is negative\\n        else:\\n            y = 0.2  # Lowest probability if both features are negative\\n\\n        # Do not change the code after this point.\\n        output.append(y)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.821757,\n",
       "    'pr_auc': 0.765302,\n",
       "    'accuracy': 0.825165,\n",
       "    'recall': 0.849195,\n",
       "    'precision': 0.810056,\n",
       "    'f1_score': 0.829164}}),\n",
       " ('model_5',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n\\n        # Simple linear model based on observation\\n        # Adjust weights and bias based on the dataset characteristics\\n        weight1 = 0.5\\n        weight2 = 0.5\\n        bias = -0.2\\n\\n        # Linear combination\\n        linear_combination = weight1 * row[0] + weight2 * row[1] + bias\\n        # Sigmoid function for probability\\n        y = 1 / (1 + np.exp(-linear_combination))\\n\\n        # Do not change the code after this point.\\n        output.append(y)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.816815,\n",
       "    'pr_auc': 0.750226,\n",
       "    'accuracy': 0.755043,\n",
       "    'recall': 0.703322,\n",
       "    'precision': 0.787179,\n",
       "    'f1_score': 0.742892}}),\n",
       " ('model_15',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n        \\n        # Simple heuristic based on observed dataset patterns\\n        if row[0] > 0 and row[1] > 0:\\n            y = 0.8  # High chance of being 1 if both features are positive\\n        elif row[0] < 0 and row[1] > 1:\\n            y = 0.7  # Also a good chance of being 1 if first feature is negative and second is significantly positive\\n        elif row[0] < -1 and row[1] < -1:\\n            y = 0.1  # Low chance of being 1 if both features are significantly negative\\n        elif row[0] > 1 and row[1] < 0:\\n            y = 0.6  # Moderate chance of being 1 if first feature is significantly positive and second is negative\\n        else:\\n            y = 0.5  # Uncertain/average chance of being 1 in other cases\\n\\n        # Do not change the code after this point.\\n        output.append(y)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.813576,\n",
       "    'pr_auc': 0.761679,\n",
       "    'accuracy': 0.76074,\n",
       "    'recall': 0.78011,\n",
       "    'precision': 0.745512,\n",
       "    'f1_score': 0.762419}}),\n",
       " ('model_3',\n",
       "  {'code_model': \"#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n\\n        # Simple linear decision boundary based on observation\\n        # Adjust the coefficients and intercept based on the dataset's pattern\\n        coef1, coef2, intercept = 0.5, 0.5, -0.25\\n        linear_combination = coef1 * row[0] + coef2 * row[1] + intercept\\n        probability = 1 / (1 + np.exp(-linear_combination))  # Sigmoid function for probability\\n\\n        y = probability\\n        # Do not change the code after this point.\\n        output.append(y)\\n    return np.array(output)\\n#########\",\n",
       "   'metric_dict': {'roc_auc': 0.793406,\n",
       "    'pr_auc': 0.70703,\n",
       "    'accuracy': 0.728433,\n",
       "    'recall': 0.655602,\n",
       "    'precision': 0.759615,\n",
       "    'f1_score': 0.703786}}),\n",
       " ('model_13',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Feature engineering and simple rule-based logic for prediction\\n        feature_1 = row[0]\\n        feature_2 = row[1]\\n        \\n        # Initialize probability of being 1 (label=1) as 0.5 as a base\\n        probability = 0.5\\n        \\n        # Adjust probability based on observed patterns\\n        if feature_1 > 0 and feature_2 > 0:\\n            probability += 0.2\\n        elif feature_1 < 0 and feature_2 > 1.5:\\n            probability += 0.3\\n        elif feature_1 > 1 and feature_2 < -1:\\n            probability -= 0.2\\n        elif feature_1 < -2 or feature_2 < -2:\\n            probability -= 0.3\\n        \\n        # Ensure probability is within [0,1]\\n        probability = max(min(probability, 1), 0)\\n        \\n        y = probability\\n        output.append(y)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.792493,\n",
       "    'pr_auc': 0.768115,\n",
       "    'accuracy': 0.767188,\n",
       "    'recall': 0.634675,\n",
       "    'precision': 0.868644,\n",
       "    'f1_score': 0.733453}}),\n",
       " ('model_19',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n        \\n        # Simple heuristic based on observed dataset patterns\\n        if row[0] > 0 and row[1] > 0:\\n            y = 0.8  # Higher probability of being 1 if both features are positive\\n        elif row[0] < 0 and row[1] > 2:\\n            y = 0.9  # Very high probability of being 1 if first feature is negative and second is greater than 2\\n        elif row[0] < -1 and row[1] < -1:\\n            y = 0.1  # Low probability of being 1 if both features are less than -1\\n        elif row[0] > 2:\\n            y = 0.7  # Moderately high probability of being 1 if first feature is greater than 2\\n        else:\\n            y = 0.5  # Neutral/uncertain probability if none of the above conditions are met\\n\\n        # Do not change the code after this point.\\n        output.append(y)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.773347,\n",
       "    'pr_auc': 0.720034,\n",
       "    'accuracy': 0.745485,\n",
       "    'recall': 0.578327,\n",
       "    'precision': 0.840753,\n",
       "    'f1_score': 0.685276}}),\n",
       " ('model_10',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n        \\n        # Simple linear model based on observed data characteristics\\n        # This is a placeholder for the actual logic which should be derived from the data\\n        # For demonstration purposes, we use a simple heuristic based on the mean values of the columns\\n        # Note: This is not an accurate model, just a demonstration of structure\\n        \\n        # Placeholder heuristic for demonstration\\n        if row[0] > 0 and row[1] > 0:\\n            y = 0.7  # Higher probability of being 1 if both features are positive\\n        elif row[0] < 0 and row[1] < 0:\\n            y = 0.3  # Lower probability of being 1 if both features are negative\\n        else:\\n            y = 0.5  # Neutral probability if features have mixed signs\\n\\n        # Do not change the code after this point.\\n        output.append(y)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.756416,\n",
       "    'pr_auc': 0.696391,\n",
       "    'accuracy': 0.667035,\n",
       "    'recall': 0.438944,\n",
       "    'precision': 0.810976,\n",
       "    'f1_score': 0.569593}}),\n",
       " ('model_11',\n",
       "  {'code_model': \"#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n        \\n        # Simple linear model based on observed data patterns\\n        # This is a placeholder for the actual logic which would normally involve\\n        # more complex calculations and possibly machine learning model predictions.\\n        # Given the constraints, we'll use a simple heuristic based on the data.\\n        \\n        # Heuristic:\\n        # If the sum of the two features is greater than a threshold, predict closer to 1,\\n        # otherwise, predict closer to 0. This is a simplistic approach and not accurate.\\n        \\n        threshold = 0.5\\n        feature_sum = row[0] + row[1]\\n        \\n        if feature_sum > threshold:\\n            y = 0.75  # Probability leaning towards 1\\n        else:\\n            y = 0.25  # Probability leaning towards 0\\n\\n        # Do not change the code after this point.\\n        output.append(y)\\n    return np.array(output)\\n#########\",\n",
       "   'metric_dict': {'roc_auc': 0.736327,\n",
       "    'pr_auc': 0.682097,\n",
       "    'accuracy': 0.736235,\n",
       "    'recall': 0.677487,\n",
       "    'precision': 0.768409,\n",
       "    'f1_score': 0.720089}}),\n",
       " ('model_8',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n        \\n        # Simple heuristic based on data observation:\\n        # If the sum of the two features is greater than a threshold, predict closer to 1, else closer to 0.\\n        # This is a naive approach and should be replaced with a proper model for real applications.\\n        \\n        feature_sum = row[0] + row[1]\\n        \\n        if feature_sum > 0:\\n            y = 0.7  # Probability leaning towards 1\\n        else:\\n            y = 0.3  # Probability leaning towards 0\\n\\n        # Do not change the code after this point.\\n        output.append(y)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.7109,\n",
       "    'pr_auc': 0.645138,\n",
       "    'accuracy': 0.710753,\n",
       "    'recall': 0.733766,\n",
       "    'precision': 0.698969,\n",
       "    'f1_score': 0.715945}}),\n",
       " ('model_6',\n",
       "  {'code_model': '#########\\ndef predict(x):\\n    import numpy as np\\n\\n    df = x.copy()\\n    output = []\\n    for index, row in df.iterrows():\\n        # Do not change the code before this point.\\n        # Please describe the process required to make the prediction below.\\n        \\n        # Simple heuristic based on data observation:\\n        # If the sum of the absolute values of the two features is less than a threshold, predict closer to 1, else closer to 0.\\n        # This is a naive approach and not based on any machine learning model.\\n        threshold = 2.5\\n        feature_sum = abs(row[0]) + abs(row[1])\\n        if feature_sum < threshold:\\n            y = 0.7  # More likely to be 1\\n        else:\\n            y = 0.3  # More likely to be 0\\n\\n        # Do not change the code after this point.\\n        output.append(y)\\n    return np.array(output)\\n#########',\n",
       "   'metric_dict': {'roc_auc': 0.478419,\n",
       "    'pr_auc': 0.499872,\n",
       "    'accuracy': 0.482382,\n",
       "    'recall': 0.670238,\n",
       "    'precision': 0.494728,\n",
       "    'f1_score': 0.569262}})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc': 0.90496, 'pr_auc': 0.878409, 'accuracy': 0.811, 'recall': 0.943907, 'precision': 0.753086, 'f1_score': 0.837768}\n",
      "{'roc_auc': 0.900645, 'pr_auc': 0.86965, 'accuracy': 0.76, 'recall': 0.959381, 'precision': 0.693706, 'f1_score': 0.805195}\n",
      "{'roc_auc': 0.897666, 'pr_auc': 0.864396, 'accuracy': 0.73, 'recall': 0.967118, 'precision': 0.664011, 'f1_score': 0.787402}\n",
      "{'roc_auc': 0.895583, 'pr_auc': 0.861315, 'accuracy': 0.7, 'recall': 0.972921, 'precision': 0.637516, 'f1_score': 0.770291}\n",
      "{'roc_auc': 0.894134, 'pr_auc': 0.85927, 'accuracy': 0.691, 'recall': 0.974855, 'precision': 0.63, 'f1_score': 0.765376}\n",
      "{'roc_auc': 0.894578, 'pr_auc': 0.86045, 'accuracy': 0.801, 'recall': 0.941973, 'precision': 0.742378, 'f1_score': 0.83035}\n",
      "{'roc_auc': 0.893893, 'pr_auc': 0.859456, 'accuracy': 0.781, 'recall': 0.94971, 'precision': 0.717836, 'f1_score': 0.817652}\n",
      "{'roc_auc': 0.893317, 'pr_auc': 0.858581, 'accuracy': 0.765, 'recall': 0.953578, 'precision': 0.700284, 'f1_score': 0.807535}\n",
      "{'roc_auc': 0.89284, 'pr_auc': 0.857804, 'accuracy': 0.755, 'recall': 0.957447, 'precision': 0.689415, 'f1_score': 0.801619}\n",
      "{'roc_auc': 0.891503, 'pr_auc': 0.855965, 'accuracy': 0.733, 'recall': 0.959381, 'precision': 0.668464, 'f1_score': 0.787927}\n",
      "{'roc_auc': 0.899356, 'pr_auc': 0.859811, 'accuracy': 0.78, 'recall': 0.94971, 'precision': 0.716788, 'f1_score': 0.816972}\n",
      "{'roc_auc': 0.891615, 'pr_auc': 0.847392, 'accuracy': 0.798, 'recall': 0.940039, 'precision': 0.739726, 'f1_score': 0.827939}\n",
      "{'roc_auc': 0.888984, 'pr_auc': 0.84432, 'accuracy': 0.797, 'recall': 0.940039, 'precision': 0.738602, 'f1_score': 0.827234}\n",
      "{'roc_auc': 0.883201, 'pr_auc': 0.834729, 'accuracy': 0.809, 'recall': 0.930368, 'precision': 0.756289, 'f1_score': 0.834345}\n",
      "{'roc_auc': 0.882196, 'pr_auc': 0.835263, 'accuracy': 0.807, 'recall': 0.924565, 'precision': 0.756329, 'f1_score': 0.832028}\n",
      "{'roc_auc': 0.882072, 'pr_auc': 0.83697, 'accuracy': 0.807, 'recall': 0.924565, 'precision': 0.756329, 'f1_score': 0.832028}\n",
      "{'roc_auc': 0.880306, 'pr_auc': 0.833047, 'accuracy': 0.811, 'recall': 0.920696, 'precision': 0.762821, 'f1_score': 0.834356}\n",
      "{'roc_auc': 0.874255, 'pr_auc': 0.827841, 'accuracy': 0.818, 'recall': 0.909091, 'precision': 0.77686, 'f1_score': 0.83779}\n",
      "{'roc_auc': 0.871191, 'pr_auc': 0.825488, 'accuracy': 0.811, 'recall': 0.895551, 'precision': 0.774247, 'f1_score': 0.830493}\n"
     ]
    }
   ],
   "source": [
    "top_model = 20\n",
    "\n",
    "for i in range(1, top_model):\n",
    "    y_pred = iblbagging.predict_(x_test, i)\n",
    "    print(iblbagging.evaluate(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
