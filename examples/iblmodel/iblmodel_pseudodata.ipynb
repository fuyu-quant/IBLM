{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fuyu-quant/IBLM/blob/main/examples/iblmodel/iblmodel_pseudodata.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install iblm --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from langchain.llms import OpenAI\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from iblm import IBLModel\n",
    "\n",
    "import os\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.16</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.64</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-1.34</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        a     b     c     d  target\n",
       "28   1.16  1.38  0.16  1.87       1\n",
       "9   -0.75 -0.71  0.05 -1.03       0\n",
       "105  0.64 -1.00 -1.35 -0.69       1\n",
       "90  -0.60 -0.50  0.10 -0.76       0\n",
       "160  1.00 -0.63 -1.34 -0.16       1"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = 4\n",
    "train_data = 80\n",
    "sampling = int(train_data/2)\n",
    "sample = 400\n",
    "seed = 3658\n",
    "\n",
    "X, y = make_classification(n_samples=sample, n_features=columns, random_state=seed)\n",
    "X = np.round(X, 2)\n",
    "y = np.round(y, 2)\n",
    "\n",
    "column_name = [letter for letter in string.ascii_lowercase[:columns]] \n",
    "\n",
    "df = pd.DataFrame(X, columns = column_name)\n",
    "df['target'] = y \n",
    "\n",
    "df_1 = df[df['target'] == 1].sample(n=sampling, random_state=seed)\n",
    "df_0 = df[df['target'] == 0].sample(n=sampling, random_state=seed)\n",
    "\n",
    "df_train = pd.DataFrame()\n",
    "df_len = len(df_1)\n",
    "for i in range(df_len):\n",
    "    df1 = pd.DataFrame([df_1.iloc[i]])\n",
    "    df0 = pd.DataFrame([df_0.iloc[i]])\n",
    "    df_train = pd.concat([df_train, df1, df0])\n",
    "\n",
    "df_train['target'] = df_train['target'].astype(int)\n",
    "df_test = df.drop(df_train.index)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.drop('target', axis=1)\n",
    "y_train = df_train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = OpenAI(temperature=0, model_name = 'gpt-4-0613')\n",
    "\n",
    "params = {'columns_name': True}\n",
    "iblm = IBLModel(llm_model = llm_model, params=params, mode = 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 2400\n",
      "\tPrompt Tokens: 2001\n",
      "\tCompletion Tokens: 399\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.08396999999999999\n"
     ]
    }
   ],
   "source": [
    "#file_path = '/content/'\n",
    "\n",
    "#model = iblm.fit(x_train, y_train, model_name = 'pseudodata', file_path=file_path)\n",
    "model = iblm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a simple Python code that uses a basic logistic regression model to predict the probability that the \"target\" of the unknown data is 1. This code does not use any existing machine learning model, but rather implements the logistic regression model from scratch.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "def sigmoid(x):\n",
      "    return 1 / (1 + np.exp(-x))\n",
      "\n",
      "def predict(x):\n",
      "    df = x.copy()\n",
      "    output = []\n",
      "    for index, row in df.iterrows():\n",
      "        # Do not change the code before this point.\n",
      "        # Please describe the process required to make the prediction below.\n",
      "        z = row['a'] + row['b'] + row['c'] + row['d']\n",
      "        y = sigmoid(z)\n",
      "        # Do not change the code after this point.\n",
      "        output.append(y)\n",
      "    return np.array(output)\n",
      "```\n",
      "\n",
      "This code first defines a helper function `sigmoid(x)` that implements the sigmoid function, which is used in logistic regression to map any real-valued number into the range [0, 1]. This function is then used in the `predict(x)` function to compute the probability that the \"target\" of the unknown data is 1.\n",
      "\n",
      "In the `predict(x)` function, for each row in the input DataFrame `df`, it computes a linear combination of the features 'a', 'b', 'c', and 'd' (i.e., `z = row['a'] + row['b'] + row['c'] + row['d']`), and then applies the sigmoid function to this linear combination to obtain the predicted probability `y = sigmoid(z)`. The predicted probabilities are collected in the list `output`, which is then converted into a NumPy array and returned.\n",
      "\n",
      "Please note that this is a very basic model and its predictive performance may not be very high. For better performance, you may want to consider using more sophisticated models and/or feature engineering techniques.\n"
     ]
    }
   ],
   "source": [
    "# Code Model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test.drop('target', axis=1)\n",
    "y_test = df_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/Library/Caches/pypoetry/virtualenvs/iblm-L6oop2Mj-py3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3508\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[99], line 1\u001b[0m\n    y_proba = iblm.predict(x_test)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/iblm-L6oop2Mj-py3.9/lib/python3.9/site-packages/iblm/iblmodel/iblm_classifier.py:110\u001b[0;36m in \u001b[0;35mpredict\u001b[0;36m\n\u001b[0;31m    exec(code, globals())\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Here is a simple Python code that uses a basic logistic regression model to predict the probability that the \"target\" of the unknown data is 1. This code does not use any existing machine learning model, but rather implements the logistic regression model from scratch.\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "y_proba = iblm.predict(x_test)\n",
    "y_pred = (y_proba > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.16\n",
      "Precision: 0.165\n",
      "Recall: 0.1717\n",
      "F1 score: 0.1683\n",
      "ROC-AUC: 0.0738\n"
     ]
    }
   ],
   "source": [
    "accuracy = round(accuracy_score(y_test, y_pred),4)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Precision\n",
    "precision = round(precision_score(y_test, y_pred),4)\n",
    "print(f'Precision: {precision}')\n",
    "\n",
    "# Recall\n",
    "recall = round(recall_score(y_test, y_pred),4)\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "# F1 score\n",
    "f1 = round(f1_score(y_test, y_pred),4)\n",
    "print(f'F1 score: {f1}')\n",
    "\n",
    "# ROC-AUC (you need prediction probabilities for this, not just class predictions)\n",
    "# Here we just reuse y_pred for simplicity\n",
    "roc_auc = round(roc_auc_score(y_test, y_proba),4)\n",
    "print(f'ROC-AUC: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('iblm-L6oop2Mj-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67008fb9323ab6c1c90f5d9822582dbef34eff0db475eec3af4e0e9456757ebf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
