{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cf756e-3fb1-4686-981f-8aa6507f34d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from iblm import IBLMClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1ff9b2-0e9d-4829-94e0-5ca5f39b9a61",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c8369-f743-4d39-905c-4a8446b5fbe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def code_model_evaluation(model_file: str, test_df: pd.DataFrame, target_colname: str) -> dict:\n",
    "    model_file = Path(model_file)\n",
    "    model_name = model_file.stem\n",
    "    import_file = f\"import models.{model_file.parent.name}.{model_name} as codemodel\"\n",
    "    with open(model_file, \"r\") as fp:\n",
    "        code = fp.read()\n",
    "\n",
    "    try:\n",
    "        exec(import_file, globals())\n",
    "    except:\n",
    "        result = dict(\n",
    "            model_name=model_name,\n",
    "            status=\"FAILED\",\n",
    "            comment=\"1_import_error\",\n",
    "            acc=None,\n",
    "            auc=None,\n",
    "            code=code,\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    try:\n",
    "        x_test = test_df.drop(target_colname, axis=1)\n",
    "        y_test = test_df[target_colname]\n",
    "        y_proba = codemodel.predict(x_test)\n",
    "        y_pred = (y_proba > 0.5).astype(int)\n",
    "        negative_values_exist = np.any(y_proba < 0)\n",
    "        values_greater_than_one_exist = np.any(y_proba > 1)\n",
    "        if negative_values_exist:\n",
    "            result = dict(\n",
    "                model_name=model_name,\n",
    "                status=\"FAILED\",\n",
    "                # comment=\"negative_values_exist\",\n",
    "                comment=\"3_invalid_pred_value\",\n",
    "                acc=None,\n",
    "                auc=None,\n",
    "                code=code,\n",
    "            )\n",
    "\n",
    "        elif values_greater_than_one_exist:\n",
    "            result = dict(\n",
    "                model_name=model_name,\n",
    "                status=\"FAILED\",\n",
    "                # comment=\"values_greater_than_one_exist\",\n",
    "                comment=\"3_invalid_pred_value\",\n",
    "                acc=None,\n",
    "                auc=None,\n",
    "                code=code,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(y_test, y_proba)\n",
    "            accuracy = round(accuracy_score(y_test, y_pred), 4)\n",
    "            result = dict(\n",
    "                model_name=model_name,\n",
    "                status=\"SUCCEEDED\",\n",
    "                comment=\"0_succeeded\",\n",
    "                acc=accuracy,\n",
    "                auc=roc_auc,\n",
    "                code=code,\n",
    "            )\n",
    "    except Exception as e:\n",
    "        result = dict(\n",
    "            model_name=model_name,\n",
    "            status=\"FAILED\",\n",
    "            comment=\"2_predict_method_error\",\n",
    "            acc=None,\n",
    "            auc=None,\n",
    "            code=code,\n",
    "        )\n",
    "\n",
    "    finally:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05babc4-bff0-4fe0-87a0-1eaeff67f137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_all_code_models(model_name: str, seeds: list, train_nums: list, target_colname: str) -> pd.DataFrame:\n",
    "    \"\"\"this is for next models\n",
    "    * moon\n",
    "    * pseudodata\n",
    "    * titanic\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for seed in tqdm(seeds):\n",
    "        df = pd.read_csv(f\"../data/{model_name}/{model_name}_{seed}_300_test.csv\")\n",
    "        for train_num in tqdm(train_nums, leave=False):\n",
    "            for trial in range(1, 31):\n",
    "                model_file = f\"../models/{model_name}/{model_name}_{seed}_{train_num}_{trial}.py\"\n",
    "                results.append(\n",
    "                    dict(\n",
    "                        seed=seed,\n",
    "                        train_num=train_num,\n",
    "                        **code_model_evaluation(model_file, df, target_colname),\n",
    "                    )\n",
    "                )\n",
    "    print(\"Done!\")\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def evaluate_all_code_models_circle(\n",
    "    model_name: str, seeds: list, train_nums: list, target_colname: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"this is for next models\n",
    "    * circle\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for seed in tqdm(seeds):\n",
    "        df = pd.read_csv(f\"../data/{model_name}/{model_name}_300_test.csv\")\n",
    "        for train_num in tqdm(train_nums, leave=False):\n",
    "            for trial in range(1, 31):\n",
    "                model_file = f\"../models/{model_name}/{model_name}_{trial}.py\"\n",
    "                results.append(\n",
    "                    dict(\n",
    "                        seed=seed,\n",
    "                        train_num=train_num,\n",
    "                        **code_model_evaluation(model_file, df, target_colname),\n",
    "                    )\n",
    "                )\n",
    "    print(\"Done!\")\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def evaluate_all_code_models_text(model_name: str, seeds: list, train_nums: list, target_colname: str) -> pd.DataFrame:\n",
    "    \"\"\"this is for next models\n",
    "    * text\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for seed in tqdm(seeds):\n",
    "        df = pd.read_csv(f\"../data/{model_name}/{model_name}_25_test.csv\")\n",
    "        for train_num in tqdm(train_nums, leave=False):\n",
    "            for trial in range(1, 31):\n",
    "                model_file = f\"../models/{model_name}/{model_name}_{trial}.py\"\n",
    "                results.append(\n",
    "                    dict(\n",
    "                        seed=seed,\n",
    "                        train_num=train_num,\n",
    "                        **code_model_evaluation(model_file, df, target_colname),\n",
    "                    )\n",
    "                )\n",
    "    print(\"Done!\")\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f2688-521d-4dee-884a-4b01d9805701",
   "metadata": {},
   "source": [
    "## dataframe processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66af702-7756-46ce-8ddf-a190f2148668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def metrics_summary(df: pd.DataFrame) -> None:  # , output_dir: str) -> None:\n",
    "    summary_df = df.groupby([\"seed\", \"train_num\"], as_index=False).agg(\n",
    "        dict(\n",
    "            status=[\"count\"],\n",
    "            acc=[\"count\", \"mean\", \"min\", \"max\"],\n",
    "            auc=[\"mean\", \"min\", \"max\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # multi-columns to single-columns\n",
    "    renamed_colnames = [\"_\".join(x).strip(\"_\") for x in summary_df.columns]\n",
    "    summary_df.columns = summary_df.columns.droplevel(0)\n",
    "    summary_df.columns = renamed_colnames\n",
    "    summary_df = summary_df.rename(columns=dict(status_count=\"n_trials\", acc_count=\"n_succeeses\"))\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def code_model_execution_trial_count_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.comment.value_counts().to_frame().T.loc[:, sorted(df.comment.unique())].reset_index(drop=True)\n",
    "    df.insert(0, \"n_trials\", df.sum(axis=1).values[0])\n",
    "    return df\n",
    "\n",
    "\n",
    "def code_model_execution_trial_count_each_case(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = (\n",
    "        df.groupby([\"seed\", \"train_num\", \"comment\"], as_index=False)\n",
    "        .agg(dict(model_name=\"count\"))\n",
    "        .rename(columns=dict(model_name=\"n_events\"))\n",
    "        .pivot(index=[\"seed\", \"train_num\"], columns=\"comment\", values=\"n_events\")\n",
    "        .fillna(\"0\")\n",
    "        .astype(int)\n",
    "        .loc[:, sorted(df.comment.unique())]\n",
    "    )\n",
    "    df.insert(0, \"n_trials\", df.sum(axis=1).values[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6dac38-9ca8-4a6f-b6f7-17e05a6862f2",
   "metadata": {},
   "source": [
    "## save dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c75356-f396-49e3-9962-a5c3d4eb0e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_raw_results(df: pd.DataFrame, output_dir: str) -> None:\n",
    "    df = df.sort_values([\"seed\", \"train_num\", \"auc\"], ascending=[True, True, False])\n",
    "    df.to_csv(f\"{output_dir}/raw_results.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "def save_metrics_summary(df: pd.DataFrame, output_dir: str) -> None:\n",
    "    df.to_csv(f\"{output_dir}/metrics_summary.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "def save_code_model_execution_trial_count_summary(df: pd.DataFrame, output_dir: str) -> None:\n",
    "    df.to_csv(f\"{output_dir}/code_model_execution_trial_count_summary.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "def save_code_model_execution_trial_count_each_case(df: pd.DataFrame, output_dir: str) -> None:\n",
    "    df.to_csv(f\"{output_dir}/code_model_execution_trial_count_each_case.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71290c39-6304-4e7d-816d-f53d3f22e203",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d2d01d-4846-4be2-b5d2-db330ba24945",
   "metadata": {},
   "source": [
    "### pseudodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e268e1c2-683c-4e10-9734-d0eb17550572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"pseudodata\"\n",
    "seeds = [3655, 3656, 3657]\n",
    "train_nums = [10, 20, 30, 40, 50, 100, 200, 300]\n",
    "target_colname = \"target\"\n",
    "\n",
    "output_dir = Path(f\"../data/code_model_evaluation/{model_name}\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df = evaluate_all_code_models(model_name, seeds, train_nums, target_colname)\n",
    "\n",
    "metrics_summary_df = metrics_summary(df)\n",
    "code_model_execution_trial_count_summary_df = code_model_execution_trial_count_summary(df)\n",
    "code_model_execution_trial_count_each_case_df = code_model_execution_trial_count_each_case(df)\n",
    "\n",
    "save_raw_results(df, output_dir)\n",
    "save_metrics_summary(metrics_summary_df, output_dir)\n",
    "save_code_model_execution_trial_count_summary(code_model_execution_trial_count_summary_df, output_dir)\n",
    "save_code_model_execution_trial_count_each_case(code_model_execution_trial_count_each_case_df, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba69c2b-cc76-44dc-988c-1711d84c9725",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T21:47:57.534971Z",
     "iopub.status.busy": "2023-07-10T21:47:57.534742Z",
     "iopub.status.idle": "2023-07-10T21:47:57.541046Z",
     "shell.execute_reply": "2023-07-10T21:47:57.537484Z",
     "shell.execute_reply.started": "2023-07-10T21:47:57.534955Z"
    },
    "tags": []
   },
   "source": [
    "### moon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff448e-ff7e-4ec1-adde-52eba4a412d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"moon\"\n",
    "seeds = [3655, 3656, 3657]\n",
    "train_nums = [10, 20, 30, 40, 50, 100, 200, 300]\n",
    "target_colname = \"target\"\n",
    "\n",
    "output_dir = Path(f\"../data/code_model_evaluation/{model_name}\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df = evaluate_all_code_models(model_name, seeds, train_nums, target_colname)\n",
    "\n",
    "metrics_summary_df = metrics_summary(df)\n",
    "code_model_execution_trial_count_summary_df = code_model_execution_trial_count_summary(df)\n",
    "code_model_execution_trial_count_each_case_df = code_model_execution_trial_count_each_case(df)\n",
    "\n",
    "save_raw_results(df, output_dir)\n",
    "save_metrics_summary(metrics_summary_df, output_dir)\n",
    "save_code_model_execution_trial_count_summary(code_model_execution_trial_count_summary_df, output_dir)\n",
    "save_code_model_execution_trial_count_each_case(code_model_execution_trial_count_each_case_df, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb18240-173e-4e18-af4e-366b2be667ae",
   "metadata": {},
   "source": [
    "### titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb0777c-f1e1-4377-8b1d-8ff191128bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"titanic\"\n",
    "seeds = [3655, 3656, 3657]\n",
    "train_nums = [6, 8, 10, 20, 30, 40, 50]\n",
    "target_colname = \"survived\"\n",
    "\n",
    "output_dir = Path(f\"../data/code_model_evaluation/{model_name}\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df = evaluate_all_code_models(model_name, seeds, train_nums, target_colname)\n",
    "\n",
    "metrics_summary_df = metrics_summary(df)\n",
    "code_model_execution_trial_count_summary_df = code_model_execution_trial_count_summary(df)\n",
    "code_model_execution_trial_count_each_case_df = code_model_execution_trial_count_each_case(df)\n",
    "\n",
    "save_raw_results(df, output_dir)\n",
    "save_metrics_summary(metrics_summary_df, output_dir)\n",
    "save_code_model_execution_trial_count_summary(code_model_execution_trial_count_summary_df, output_dir)\n",
    "save_code_model_execution_trial_count_each_case(code_model_execution_trial_count_each_case_df, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29139d60-a5cc-4346-8efc-483f94d6eae9",
   "metadata": {},
   "source": [
    "### circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872db863-9eec-4b8f-bb72-d64eae24ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"circle\"\n",
    "seeds = [\"\"]\n",
    "train_nums = [\"\"]\n",
    "target_colname = \"Target\"\n",
    "\n",
    "output_dir = Path(f\"../data/code_model_evaluation/{model_name}\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df = evaluate_all_code_models_circle(model_name, seeds, train_nums, target_colname)\n",
    "\n",
    "metrics_summary_df = metrics_summary(df)\n",
    "code_model_execution_trial_count_summary_df = code_model_execution_trial_count_summary(df)\n",
    "code_model_execution_trial_count_each_case_df = code_model_execution_trial_count_each_case(df)\n",
    "\n",
    "save_raw_results(df, output_dir)\n",
    "save_metrics_summary(metrics_summary_df, output_dir)\n",
    "save_code_model_execution_trial_count_summary(code_model_execution_trial_count_summary_df, output_dir)\n",
    "save_code_model_execution_trial_count_each_case(code_model_execution_trial_count_each_case_df, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71567863-5764-4383-994d-ccbd469926ee",
   "metadata": {},
   "source": [
    "### text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83a76b-46c8-465b-bd96-0f78dd6a8550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"text\"\n",
    "seeds = [\"\"]\n",
    "train_nums = [\"\"]\n",
    "target_colname = \"Target\"\n",
    "\n",
    "output_dir = Path(f\"../data/code_model_evaluation/{model_name}\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df = evaluate_all_code_models_text(model_name, seeds, train_nums, target_colname)\n",
    "\n",
    "metrics_summary_df = metrics_summary(df)\n",
    "code_model_execution_trial_count_summary_df = code_model_execution_trial_count_summary(df)\n",
    "code_model_execution_trial_count_each_case_df = code_model_execution_trial_count_each_case(df)\n",
    "\n",
    "save_raw_results(df, output_dir)\n",
    "save_metrics_summary(metrics_summary_df, output_dir)\n",
    "save_code_model_execution_trial_count_summary(code_model_execution_trial_count_summary_df, output_dir)\n",
    "save_code_model_execution_trial_count_each_case(code_model_execution_trial_count_each_case_df, output_dir)"
   ]
  }
 ],
 "metadata": {
  "autoscrollcelloutput": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
