{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install --upgrade iblm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iblm                         0.3.30\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list | grep iblm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.iblm.ibl import IBLModel\n",
    "from iblm import ibl\n",
    "\n",
    "from src.data import pseudodata\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 円形データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circle_data(n_points, num_train_data, radius, noise):\n",
    "    np.random.seed(0)\n",
    "    theta = np.random.uniform(0, 2*np.pi, n_points)\n",
    "    r = radius + np.random.normal(0, noise, n_points)  # ノイズを加える\n",
    "    x0 = r * np.cos(theta)\n",
    "    y0 = r * np.sin(theta)\n",
    "    labels0 = np.zeros(n_points)\n",
    "\n",
    "    # クラス1のデータ生成\n",
    "    theta = np.random.uniform(0, 2*np.pi, n_points)\n",
    "    r = radius * 0.5 + np.random.normal(0, noise, n_points)  # 内側の円で、ノイズを加える\n",
    "    x1 = r * np.cos(theta)\n",
    "    y1 = r * np.sin(theta)\n",
    "    labels1 = np.ones(n_points)\n",
    "\n",
    "    # データセットを結合\n",
    "    x = np.vstack((np.column_stack((x0, y0)), np.column_stack((x1, y1))))\n",
    "    y = np.hstack((labels0, labels1)).astype(int)\n",
    "\n",
    "    # データの順番をシャッフル\n",
    "    indices = np.random.permutation(len(y))\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    x = np.round(x, decimals=3)\n",
    "    x = pd.DataFrame(x)\n",
    "\n",
    "    x_train = x[0:num_train_data]\n",
    "    x_test = x[num_train_data:]\n",
    "\n",
    "    y_train = y[:num_train_data]\n",
    "    y_test = y[num_train_data:]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "radius=10\n",
    "noise=0.1\n",
    "\n",
    "x_train, x_test, y_train, y_test = circle_data(n_points=sample, num_train_data=num_train_data, radius=radius, noise=noise)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMデータセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_dataset(n_points, num_train_data):\n",
    "    np.random.seed(0)  # 再現性のためのシード設定\n",
    "\n",
    "    # クラス1とクラス2のデータ生成\n",
    "    x1_1 = np.random.normal(loc=0.5, scale=0.5, size=n_points)\n",
    "    x2_1 = np.random.normal(loc=0.5, scale=0.5, size=n_points)\n",
    "    y1 = np.ones(n_points)  # クラス1のラベル\n",
    "\n",
    "    x1_2 = np.random.normal(loc=-0.2, scale=0.5, size=n_points)\n",
    "    x2_2 = np.random.normal(loc=-0.2, scale=0.5, size=n_points)\n",
    "    y2 = np.zeros(n_points)  # クラス2のラベル\n",
    "\n",
    "    # データの結合\n",
    "    x1 = np.concatenate([x1_1, x1_2])\n",
    "    x2 = np.concatenate([x2_1, x2_2])\n",
    "    x = np.vstack([x1, x2]).T\n",
    "    y = np.concatenate([y1, y2])\n",
    "\n",
    "    # データのシャッフル\n",
    "    indices = np.random.permutation(n_points * 2)\n",
    "    x_shuffled = x[indices]\n",
    "    y_shuffled = y[indices]\n",
    "\n",
    "    # pandas DataFrameへの変換\n",
    "    x_shuffled = pd.DataFrame(x_shuffled)\n",
    "\n",
    "    # トレーニングセットとテストセットに分割\n",
    "    x_train = x_shuffled.iloc[:num_train_data]\n",
    "    x_test = x_shuffled.iloc[num_train_data:]\n",
    "    y_train = y_shuffled[:num_train_data]\n",
    "    y_test = y_shuffled[num_train_data:]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = svm_dataset(n_points = sample, num_train_data = num_train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDTデータセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_dataset(n_points, num_train_data):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Generate a mix of sine and cosine functions with noise as features\n",
    "    X1 = np.linspace(0, 6*np.pi, n_points)\n",
    "    X2 = np.sin(X1) + np.random.normal(0, 0.1, n_points)\n",
    "    X3 = np.cos(X1) + np.random.normal(0, 0.1, n_points)\n",
    "    X4 = X2 * X3 + np.random.normal(0, 0.1, n_points)  # Interaction term\n",
    "    x = np.vstack((X2, X3, X4)).T\n",
    "\n",
    "    # Generate labels based on a non-linear combination of features\n",
    "    y = (np.sin(X1) > 0).astype(int)\n",
    "\n",
    "    # Fix: Corrected the way of shuffling data using a single sequence of indices\n",
    "    indices = np.arange(n_points)  # Fixed to generate indices for the actual number of points\n",
    "    np.random.shuffle(indices)\n",
    "    x_shuffled = x[indices]\n",
    "    y_shuffled = y[indices]\n",
    "\n",
    "    # Conversion to pandas DataFrame\n",
    "    x_shuffled = pd.DataFrame(x_shuffled, columns=['Feature1', 'Feature2', 'Feature3'])\n",
    "\n",
    "    # Splitting into training and test sets\n",
    "    x_train = x_shuffled.iloc[:num_train_data]\n",
    "    x_test = x_shuffled.iloc[num_train_data:]\n",
    "    y_train = y_shuffled[:num_train_data]\n",
    "    y_test = y_shuffled[num_train_data:]\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = lgbm_dataset(n_points = sample, num_train_data = num_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn擬似データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_data = 500 # 50〜300\n",
    "num_test_data = 1000\n",
    "sample = num_train_data + num_test_data\n",
    "n_informative = 2\n",
    "n_redundant = 0\n",
    "n_features = n_informative + n_redundant\n",
    "weights = [0.5, 0.5]\n",
    "flip_y=0\n",
    "seed = 3655  # 3655,3656,3657\n",
    "\n",
    "# testデータの個数を揃えるためにtrain_test_splitを使っていない\n",
    "x, y = make_classification(\n",
    "    n_samples = sample,  # データ数\n",
    "    n_features = n_features,  # 特徴量の数\n",
    "    n_informative = n_informative,  # ラベル予測に意味のある特徴量の数\n",
    "    n_redundant = n_redundant,  # 冗長な特徴量\n",
    "    weights = weights,  # [0,1]の割合\n",
    "    flip_y = flip_y, # 逆のラベルに反転する割合\n",
    "    random_state = seed\n",
    "    )\n",
    "\n",
    "x = np.round(x, decimals=3)\n",
    "x = pd.DataFrame(x)\n",
    "\n",
    "x_test = x[0:num_test_data]\n",
    "x_train = x[num_test_data:]\n",
    "\n",
    "y_test = y[:num_test_data]\n",
    "y_train = y[num_test_data:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 21:21:20,455 [iblm.ibl][INFO] (ibl:ibl.py:fit:151)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.9918074756784434,\n",
       " 'pr_auc': 0.9893431639830674,\n",
       " 'accuracy': 0.81,\n",
       " 'recall': 1.0,\n",
       " 'precision': 0.7262247838616714,\n",
       " 'f1_score': 0.8414023372287145}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iblm = ibl.IBLModel(api_type=\"openai\", model_name=\"gpt-4-0125-preview\", objective=\"binary\")\n",
    "model = iblm.fit(x_train, y_train, seed=3655)\n",
    "y_pred = iblm.predict(x_test)\n",
    "iblm.evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########\n",
      "def predict(x):\n",
      "    import numpy as np\n",
      "\n",
      "    df = x.copy()\n",
      "    output = []\n",
      "    for index, row in df.iterrows():\n",
      "        # Do not change the code before this point.\n",
      "        # Please describe the process required to make the prediction below.\n",
      "        \n",
      "        # Simple logistic regression coefficients derived from the dataset manually or through a simple algorithm\n",
      "        # These coefficients are placeholders and should ideally be determined through analysis of the dataset\n",
      "        # For demonstration purposes only\n",
      "        intercept = 0.5\n",
      "        coef_col0 = 0.8\n",
      "        coef_col1 = -0.3\n",
      "        \n",
      "        # Logistic regression formula\n",
      "        z = intercept + (coef_col0 * row[0]) + (coef_col1 * row[1])\n",
      "        y = 1 / (1 + np.exp(-z))\n",
      "\n",
      "        # Do not change the code after this point.\n",
      "        output.append(y)\n",
      "    return np.array(output)\n",
      "#########\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########\n",
      "def predict(x):\n",
      "    import numpy as np\n",
      "\n",
      "    df = x.copy()\n",
      "    output = []\n",
      "    for index, row in df.iterrows():\n",
      "        # Simple logistic regression coefficients are manually estimated based on the dataset characteristics.\n",
      "        # These coefficients are placeholders and should ideally be determined using a logistic regression model fitting process.\n",
      "        # Intercept\n",
      "        b0 = -0.5\n",
      "        # Coefficients for the features\n",
      "        b1 = 0.8\n",
      "        b2 = 0.2\n",
      "\n",
      "        # Logistic regression model\n",
      "        z = b0 + b1*row[0] + b2*row[1]\n",
      "        y = 1 / (1 + np.exp(-z))\n",
      "\n",
      "        output.append(y)\n",
      "    return np.array(output)\n",
      "#########\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-12 06:07:48,926 [iblm.ibl][INFO] (ibl:ibl.py:fit:154)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.24206596888803533,\n",
       " 'pr_auc': 0.36019643762895803,\n",
       " 'accuracy': 0.317,\n",
       " 'recall': 0.3333333333333333,\n",
       " 'precision': 0.3159922928709056,\n",
       " 'f1_score': 0.32443125618199803}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iblm = ibl.IBLModel(api_type=\"openai\", model_name=\"gpt-3.5-turbo-0125\", objective=\"binary\")\n",
    "model = iblm.fit(x_train, y_train, seed=3655)\n",
    "y_pred = iblm.predict(x_test)\n",
    "iblm.evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 21:26:00,737 [iblm.ibl][INFO] (ibl:ibl.py:fit:151)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.8604810707885304,\n",
       " 'pr_auc': 0.8908559550390702,\n",
       " 'accuracy': 0.755,\n",
       " 'recall': 0.751984126984127,\n",
       " 'precision': 0.7595190380761523,\n",
       " 'f1_score': 0.7557328015952144}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iblm = ibl.IBLModel(api_type=\"gemini\", model_name=\"gemini-pro\", objective=\"binary\")\n",
    "model = iblm.fit(x_train, y_train)\n",
    "y_pred = iblm.predict(x_test)\n",
    "iblm.evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########\n",
      "def predict(x):\n",
      "    import numpy as np\n",
      "\n",
      "    df = x.copy()\n",
      "    output = []\n",
      "    for index, row in df.iterrows():\n",
      "        # Do not change the code before this point.\n",
      "        # Please describe the process required to make the prediction below.\n",
      "        y = 1 / (1 + np.exp(-(row[0] + row[1])))\n",
      "\n",
      "\n",
      "        # Do not change the code after this point.\n",
      "        output.append(y)\n",
      "    return np.array(output)\n",
      "#########\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ロジスティック回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression AUC: 0.9724\n",
      "Logistic Regression Accuracy: 0.9360\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state=3655)\n",
    "log_reg.fit(x_train, y_train)\n",
    "log_reg_probs = log_reg.predict_proba(x_test)[:, 1]\n",
    "log_reg_auc = roc_auc_score(y_test, log_reg_probs)\n",
    "\n",
    "\n",
    "log_reg_preds = log_reg.predict(x_test) # 予測ラベルを計算\n",
    "log_reg_accuracy = accuracy_score(y_test, log_reg_preds) # 精度を計算\n",
    "\n",
    "# AUCと精度を出力\n",
    "print(f\"Logistic Regression AUC: {log_reg_auc:.4f}\")\n",
    "print(f\"Logistic Regression Accuracy: {log_reg_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM AUC: 0.9807\n",
      "SVM Accuracy: 0.9360\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(probability=True, random_state=42)  # probability=Trueで確率出力を有効化\n",
    "svm.fit(x_train, y_train)\n",
    "svm_probs = svm.predict_proba(x_test)[:, 1]\n",
    "svm_auc = roc_auc_score(y_test, svm_probs)\n",
    "\n",
    "# 追加コード: 精度を計算\n",
    "svm_preds = svm.predict(x_test) # 予測ラベルを計算\n",
    "svm_accuracy = accuracy_score(y_test, svm_preds) # 精度を計算\n",
    "\n",
    "# AUCと精度を出力\n",
    "print(f\"SVM AUC: {svm_auc:.4f}\")\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT AUC: 0.9814\n",
      "GBDT Accuracy: 0.9240\n"
     ]
    }
   ],
   "source": [
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_clf.fit(x_train, y_train)\n",
    "y_pred = gb_clf.predict_proba(x_test)[:, 1]\n",
    "gbdt_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# 追加コード: 精度を計算\n",
    "gb_clf_preds = gb_clf.predict(x_test) # 予測ラベルを計算\n",
    "gb_clf_accuracy = accuracy_score(y_test, gb_clf_preds) # 精度を計算\n",
    "\n",
    "# AUCと精度を出力\n",
    "print(f\"GBDT AUC: {gbdt_auc:.4f}\")\n",
    "print(f\"GBDT Accuracy: {gb_clf_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values = [0.99180, 0.93893, 0.92402, 0.93959]\n",
    "#values = [0.87694, 0.881488, 0.95156, 0.90104]\n",
    "#values = [0.860481, 0.841916, 0.862391, 0.8450683]\n",
    "\n",
    "\n",
    "#values = [0.72598, 0.85823, 0.85836, 0.91805, 0.99180]\n",
    "#values = [0.75880, 0.90408, 0.88149, 0.93893, 0.83601]\n",
    "#values = [0.67326, 0.92402, 0.650133, 0.223092, 0.85594]\n",
    "#values = [0.93959, 0.90104, 0.72344, 0.845402, 0.91754]\n",
    "\n",
    "#values = [0.75947, 0.87694, 0.809129, 0.29116, 0.706267]\n",
    "#values = [0.12979, 0.80999, 0.881488, 0.84443, 0.32652]\n",
    "#values = [0.29293, 0.931014, 0.95156, 0.85095, 0.727106]\n",
    "#values = [0.68948, 0.90104, 0.310713, 0.84540, 0.2420659]\n",
    "\n",
    "#values = [0.8128, 0.85822, 0.809121, 0.859184, 0.860481]\n",
    "#values = [0.84044, 0.80999, 0.796973, 0.841916, 0.836025]\n",
    "#values = [0.862391, 0.83288, 0.834107, 0.85095, 0.855949]\n",
    "values = [0.844427, 0.818141, 0.84441, 0.84540, 0.8450683]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83948926\n",
      "0.010680877287489081\n"
     ]
    }
   ],
   "source": [
    "mean_value = sum(values) / len(values)\n",
    "print(mean_value)\n",
    "# 各値の平均値からの差の二乗の平均（分散）を計算\n",
    "variance = sum((x - mean_value) ** 2 for x in values) / len(values)\n",
    "\n",
    "# 分散の平方根を取って標準偏差を求める\n",
    "standard_deviation = variance ** 0.5\n",
    "print(standard_deviation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
